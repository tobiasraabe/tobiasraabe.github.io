
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hi, my name is Tobias. I am a data scientist and programmer living in Hamburg.\nI work on topics at the intersection of economics, statistics, and computer science, like human capital or epidemiological models.\nI am also interested in productivity, best practices, and automation. Therefore, I developed pytask, a workflow management system for reproducible research.\n  Download my resum√© .\n","date":1653523200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653523200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi, my name is Tobias. I am a data scientist and programmer living in Hamburg.\nI work on topics at the intersection of economics, statistics, and computer science, like human capital or epidemiological models.","tags":null,"title":"Tobias Raabe","type":"authors"},{"authors":["Tobias Raabe"],"categories":[],"content":"   A workflow management system for reproducible data analyses\n Why do you need pytask? You‚Äôve probably organized your project using a similar folder structure where each folder contains scripts carrying out specific tasks.\nBut, how do you execute all tasks or keep your project in sync?\n   How does pytask help you?  By defining dependencies and products of tasks, you implicitly define an execution order. pytask validates this definition and executes only tasks which need to be updated.   How do you define tasks, dependencies, and products? Tasks are functions starting with task_. Use decorators to specify the dependencies and products of a task. Using depends_on and produces as function args, you access the paths to the files in the function body.\n   Execute a task Type pytask in your terminal, and it will automatically collect and execute all tasks.\n   What are the benefits? üëâ Automation reduces errors and increases reproducibility.\nüëâ The build process is documented in code.\nüëâ You can iterate faster and be more productive.\n Research Is pytask used for actual research? Yes!\nHere is a Covid-19 forecast project with an agent-based model, 10+ datasets, many different policy scenarios and 1,000+ simulations.\nüëâ https://arxiv.org/abs/2106.11129\n Teaching pytask is also part of a graduate course, teaching economists programming and best practices for research projects.\nüëâ https://github.com/OpenSourceEconomics/econ-project-templates\n What else has pytask to offer? Scale your project by repeating tasks! üöÄ\nFor example, create ten different datasets with randomly generated data.\n   Customize pytask with plugins!  Automatically parallelize the execution with https://github.com/pytask-dev/pytask-parallel ‚ö°Ô∏è Support for executing R, Julia, and Stata scripts. All plugins are here: https://pytask-dev.readthedocs.io/en/stable/plugin_list.html   Templating Start a new project from a template!\nA minimal template: https://github.com/pytask-dev/cookiecutter-pytask-project\nA template for reproducible economics projects: https://github.com/OpenSourceEconomics/econ-project-templates\n Debugging Enter the debugger if one of your tasks fails, and you want to find out why! üèóÔ∏è\n   Documentation You can find out more about pytask in the documentation: https://pytask-dev.readthedocs.io/.\nFollow the tutorials for a step-by-step introduction: https://pytask-dev.readthedocs.io/en/stable/tutorials/index.html\n Ecosystem pytask is also part of a more extensive ecosystem of research tools developed at @open_econ.\nWe will soon write about tools like estimagic, a package for complex numerical optimization, and estimation/calibration of scientific models.\n Acknowledgements Thanks for staying with me until the end! At last, some shout-outs to amazing people and projects.\nThanks to @kroehrl, @JanosGabler, and @econ_hmg, who helped me build this tool in endless and fruitful discussions! üôá\n Acknowledgements pytask stands on the shoulders of these projects. Thank you!üôè\n @pytestdotorg for pytest and pluggy. @textualizeio for cli interface created with rich. @_darrenburns for parametrizations burrowed from ward.  ","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653523200,"objectID":"8ee07711f684c823004c1f6da4a4286b","permalink":"https://tobiasraabe.github.io/slides/pytask/","publishdate":"2022-05-26T00:00:00Z","relpermalink":"/slides/pytask/","section":"slides","summary":"An short introduction to pytask.","tags":["pytask"],"title":"pytask","type":"slides"},{"authors":null,"categories":null,"content":"GETTSIM provides a depiction of the German Taxes and Transfers System that is usable in a wide array of research applications, ranging from complex dynamic programming models to detailed microsimulation studies.\nI spent a short time on this project and created the DAG-based backend which allows users to define functions to compute quantities of a tax and transfer system and the backend will figure out the execution order itself.\nThis allows users to flexibly define their tax and transfer system or modify an existing system by replacing some functions.\nThe DAG-based backend was finally extracted and transferred into its own package so that everyone can use it and embed into their application. You can find it here: https://github.com/OpenSourceEconomics/dags.\n","date":1652918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652918400,"objectID":"15634cfcef6ac4145cda0a201bc7117c","permalink":"https://tobiasraabe.github.io/project/gettsim/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/project/gettsim/","section":"project","summary":"GETTSIM provides a depiction of the German Taxes and Transfers System that is usable in a wide array of research applications, ranging from complex dynamic programming models to detailed microsimulation studies.","tags":["Structural Econometrics","Simulation"],"title":"gettsim","type":"project"},{"authors":null,"categories":null,"content":"","date":1652918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652918400,"objectID":"d8ec893124f05809e1f478f85b331db4","permalink":"https://tobiasraabe.github.io/project/pytask/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/project/pytask/","section":"project","summary":"A workflow management system which facilitates reproducible data analyses.","tags":["Reproducibility","Python","Workflow"],"title":"pytask","type":"project"},{"authors":null,"categories":null,"content":"","date":1652918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652918400,"objectID":"13997d664234bf8f356a695a044d9d46","permalink":"https://tobiasraabe.github.io/project/respy/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/project/respy/","section":"project","summary":"A Framework for the simulation and estimation of some finite-horizon discrete choice dynamic programming models.","tags":["Human Capital","Discrete Choice","Structural Econometrics"],"title":"respy","type":"project"},{"authors":null,"categories":null,"content":"","date":1652918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652918400,"objectID":"7e12fb847d625295dc4cfc90e68da66f","permalink":"https://tobiasraabe.github.io/project/sid/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/project/sid/","section":"project","summary":"An agent-based simulation for infectious diseases like COVID-19.","tags":["COVID-19","Simulation"],"title":"sid","type":"project"},{"authors":["Jano≈õ Gabler","Tobias Raabe","Klara R√∂hrl","Hans-Martin von Gaudecker"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ","date":1625184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625184000,"objectID":"50450628d0f4be497b35f43e16c21755","permalink":"https://tobiasraabe.github.io/publication/sid/","publishdate":"2021-07-02T00:00:00Z","relpermalink":"/publication/sid/","section":"publication","summary":"In order to slow the spread of the CoViD-19 pandemic, governments around the world have enacted a wide set of policies limiting the transmission of the disease. Initially, these focused on non-pharmaceutical interventions; more recently, vaccinations and large-scale rapid testing have started to play a major role. The objective of this study is to explain the quantitative effects of these policies on determining the course of the pandemic, allowing for factors like seasonality or virus strains with different transmission profiles. To do so, the study develops an agent-based simulation model, which explicitly takes into account test demand and behavioral changes following positive tests. The model is estimated using data for the second and the third wave of the CoViD-19 pandemic in Germany. The paper finds that during a period where vaccination rates rose from 5% to 40%, seasonality and rapid testing had the largest effect on reducing infection numbers. Frequent large-scale rapid testing should remain part of strategies to contain CoViD-19; it can substitute for many non-pharmaceutical interventions that come at a much larger cost to individuals, society, and the economy.","tags":[],"title":"The Effectiveness of Testing, Vaccinations and Contact Restrictions for Containing the CoViD-19 Pandemic","type":"publication"},{"authors":["Tobias Raabe"],"categories":["python"],"content":"Hi everybody,\nI assume that all of you write tests for Python programs with pytest. If you do not use pytest or if you do not even write tests, you should check out the following links which are useful and provide some examples and an overview of pytest‚Äôs capabilities.\n Effective Python Testing With Pytest - Real Python Customizing your pytest suite (Part 1) - Raphael Pierzina Customizing your pytest suite (Part 2) - Raphael Pierzina  Maybe you should also have heard about test driven development (TDD), but I have little experience with it myself. If you have a great resource for beginners, send it my way and I can include it here.\nWhat I did not find in these guides is a combination of patterns I use fairly often to write tests. Hopefully, it is useful for you as well. Let‚Äôs go!\nThe function First, here is the function we are going to test. The function takes any number of paths and tries to find the longest parent path common to all paths.\ndef find_common_ancestor(*paths: Union[str, Path]) -\u0026gt; Path: \u0026#34;\u0026#34;\u0026#34;Find a common ancestor of many paths.\u0026#34;\u0026#34;\u0026#34; paths = [path if isinstance(path, PurePath) else Path(path) for path in paths] for path in paths: if not path.is_absolute(): raise ValueError( f\u0026#34;Cannot find common ancestor for relative paths. {path}is relative.\u0026#34; ) common_parents = set.intersection(*[set(path.parents) for path in paths]) if len(common_parents) == 0: raise ValueError(\u0026#34;Paths have no common ancestor.\u0026#34;) else: longest_parent = sorted(common_parents, key=lambda x: len(x.parts))[-1] return longest_parent Here is an example:\n\u0026gt;\u0026gt;\u0026gt; find_common_ancestor(\u0026#34;C:\\\\Users\\\\Tobias\u0026#34;, \u0026#34;C:\\\\Users\\\\Tobi\u0026#34;) WindowsPath(\u0026#39;C:/Users\u0026#39;) The function returns errors if ‚Ä¶\n one of the paths is relative. the paths do not have a common ancestor.  The test function I will first show you the test function and, then, comment on some details.\nfrom contextlib import ExitStack as does_not_raise # noqa: N813 from pathlib import Path from pathlib import PurePosixPath from pathlib import PureWindowsPath import pytest @pytest.mark.unit @pytest.mark.parametrize( \u0026#34;path_1, path_2, expectation, expected\u0026#34;, [ pytest.param( PurePosixPath(\u0026#34;relative_1\u0026#34;), PurePosixPath(\u0026#34;/home/relative_2\u0026#34;), pytest.raises( ValueError, match=\u0026#34;Cannot find common ancestor for relative paths.\u0026#34; ), None, id=\u0026#34;test path 1 is relative\u0026#34;, ), pytest.param( PureWindowsPath(\u0026#34;C:/home/relative_1\u0026#34;), PureWindowsPath(\u0026#34;relative_2\u0026#34;), pytest.raises( ValueError, match=\u0026#34;Cannot find common ancestor for relative paths.\u0026#34; ), None, id=\u0026#34;test path 2 is relative\u0026#34;, ), pytest.param( PurePosixPath(\u0026#34;/home/user/folder_a\u0026#34;), PurePosixPath(\u0026#34;/home/user/folder_b/sub_folder\u0026#34;), does_not_raise(), PurePosixPath(\u0026#34;/home/user\u0026#34;), id=\u0026#34;normal behavior with UNIX paths\u0026#34;, ), pytest.param( PureWindowsPath(\u0026#34;C:\\\\home\\\\user\\\\folder_a\u0026#34;), PureWindowsPath(\u0026#34;C:\\\\home\\\\user\\\\folder_b\\\\sub_folder\u0026#34;), does_not_raise(), PureWindowsPath(\u0026#34;C:\\\\home\\\\user\u0026#34;), id=\u0026#34;normal behavior with Windows paths\u0026#34;, ), pytest.param( PureWindowsPath(\u0026#34;C:\\\\home\\\\user\\\\folder_a\u0026#34;), PureWindowsPath(\u0026#34;D:\\\\home\\\\user\\\\folder_b\\\\sub_folder\u0026#34;), pytest.raises(ValueError, match=\u0026#34;Paths have no common ancestor.\u0026#34;), None, id=\u0026#34;no common ancestor\u0026#34;, ), ], ) def test_find_common_ancestor(path_1, path_2, expectation, expected): with expectation: result = find_common_ancestor(path_1, path_2) assert result == expected   Use pytest.mark.parametrize to minimize the test code and to make adding more tests easier.\n  Use pytest.param to wrap each iteration. It allows to add the id parameter to each iteration. Use the id to document the specific test case. With many test cases, you will quickly forget the purpose of each single test.\n  The third argument of the parametrization, expectation, can be used to assert that the tested function throws an exception. In case no exception is thrown, use does_not_raise().\n  If you expect an exception, you can pick an arbitrary object as the expected output.\n  Conclusion I hope you enjoyed this tutorial. Feel free to send me any feedback.\nPS: When I started writing this guide, I discovered this function. Maybe I do not need my implementation plus the tests.\n","date":1617148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617148800,"objectID":"e777accc0c197ef1c6eb1f432f530264","permalink":"https://tobiasraabe.github.io/post/how-i-write-tests/","publishdate":"2021-03-31T00:00:00Z","relpermalink":"/post/how-i-write-tests/","section":"post","summary":"Here are some useful patterns for writing tests with pytest which help you to scale your test suite.","tags":["pytest","python","tricks","testing"],"title":"How I write tests","type":"post"},{"authors":["Tobias Raabe"],"categories":["python"],"content":"Hi everybody,\nI have not written a post in a long time, so I am resuming this blog by keeping you posted on some of my more recent projects.\nI have been developing or contributing to several research applications last year. I learned a lot about software engineering and designing applications. Maybe I will find the time to make a post on some of the things I learned along the way.\npytask The project I am most excited about is pytask, a build system designed for researchers to run their project pipeline from data preparation over analyses to compiling the reports.\nI was highly frustrated with existing solutions and programmed my build system. The interface is one of the highlights. It is similar to pytest to lower the entry barrier but more beautiful because it uses rich. pytask uses pluggy under the hood to offer a plugin system.\nIf you already know my cookiecutter for reproducible research, you know Waf. pytask replaces Waf. I will probably not update the cookiecutter for the foreseeable future and, instead, I recommend Hans-Martin‚Äôs cookiecutter, which will support pytask.\nPlease take a look at pytask and try it out in your next project. I appreciate any feedback, comments, feature requests, and harsh criticism :).\nI already held a presentation about pytask‚Äôs design which I will probably post here in some weeks. Half the time is about plugin architectures in general and pluggy, and the other half is about pytask.\nrespy Together with Janos, I created a framework for a certain class of econometric models called respy.\nFor the insiders, it is a framework for finite-horizon discrete choice dynamic programming models, also called Eckstein-Keane-Wolpin models. Researchers use them to study the human capital accumulation process in the labor market.\nThe documentation is quite extensive for such a young project‚Äîcontributors taking over the project plan to extend it with even more examples and applications.\nIf you are an economist interested in structural modeling, it might be an excellent place to start. Even if you do not want to use this model, you might be able to get some inspiration for your model.\nI learned a lot about building interfaces people can use and how to write performant code by choosing the right design and Numba where necessary.\ngettsim I redesigned the computational backend of gettsim with Janos and Hans-Martin. gettsim offers a representation of the German tax and transfer system and allows researchers to study the impact of reforms on the amount of taxes and benefits people face.\nThe task was to design an interface that allows users to modify or extend the pre-implemented tax and transfer system.\nOur solution is a mixture inspired by pytest‚Äôs fixtures and a DAG (directed acyclic graph).\n  A subset of the German tax and transfer system.  You can view the whole tax and transfer system as an extensive network. In this network, nodes are quantities like child benefits, capital gains, or taxes on capital gains. Edges represent how quantities relate to each other. For example, taxes paid on capital gains is derived from capital gains subject to income tax. Quantities are part of the data, or a function exists that computes it.\n The network is a directed graph because edges point in one direction. And it is acyclic since there are no cycles in this graph, meaning you will never return to the same node following the edges. These properties make the network a directed acyclic graph or a DAG.   This network view has a couple of benefits.\n  A quantity can be computed once and then passed to the following nodes saving runtime and reducing code duplication.\n  If you want to model a policy change, you can single out the relevant nodes in the network and modify the underlying functions.\n  If you are interested only in a subset of tax and transfer system, subset the network and remove unnecessary nodes.\n  This flexibility is highly desirable, but what does the interface look like for a user.\nHere, we use the idea of pytest‚Äôs fixtures where using a fixture‚Äôs name as an argument in a test function gives you access to the return of the fixture inside the test function. Similarly, a function in gettsim looks like this.\ndef child_benefits(n_children, parameters): return n_children * parameters[\u0026#34;child_benefits\u0026#34;] Here, n_children is either a variable in the input data or a function with the same name which computes the quantity.\nWe can build a DAG that allows us to determine an execution order for the functions from a function‚Äôs name and its argument names.\nUsers can modify the collection of functions by overwriting existing functions or adding their own.\nYou can find out more about the package in the documentation or check out the code to build a DAG in the standalone package dags.\nsid Last but not least, I have been working on an epidemiological model to predict the spread of infectious diseases. It is my COVID-19 project with Klara and Janos. It is called sid, and we hope to publish something soon.\n","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"11abb7f9218c8b68e9f17fccb6a9890d","permalink":"https://tobiasraabe.github.io/post/what-i-have-been-doing-lately/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/post/what-i-have-been-doing-lately/","section":"post","summary":"Here is an overview on the projects I have worked on for some time: pytask, respy, gettsim, and sid.","tags":["python","pytask","sid","gettsim","respy"],"title":"What I have been doing lately","type":"post"},{"authors":["Tobias Raabe"],"categories":["python"],"content":"%matplotlib inline import matplotlib import matplotlib.pyplot as plt import numpy as np Introduction This post shows you how to make publication-ready plots with matplotlib.\nmatplotlibrc or style sheets To align most stylistic choices, use a matplotlibrc file or style sheets. The difference between the two is that matplotlibrc is picked up from some locations, first your working directory, and automatically applied to your plots.\nIf you want to have a more dynamic approach, use style sheets which look like the following:\n# content of file style.mplstyle axes.axisbelow : True # Draw axis grid lines and ticks below patches (True); above # patches but below lines (\u0026#39;line\u0026#39;); or above all (False). # Forces grid lines below figures. font.size : 12 # Font size in pt. grid.linewidth : 1.2 # In pt. legend.framealpha : 1 # Legend patch transparency. legend.scatterpoints : 3 # Number of scatter points in legend. lines.linewidth : 3 # line width in pt. First, here is the plot with the default settings.\nx = np.linspace(0, 10, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) plt.show() plt.close()    Now, apply the style and see the changes.\nplt.style.use(\u0026#34;../style.mplstyle\u0026#34;) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) plt.show() plt.close()    There are some other things you need to know about style sheets:\n You can use all the settings from matplotlibrc. You can load multiple style sheets with plt.style.use(\u0026#34;first_style\u0026#34;, \u0026#34;second_style\u0026#34;) where overlapping options are overwritten by the following style. I have made the observation that in notebooks you better load the style in a separate line to the imports because sometimes the changes are not picked up.  The font We would like the font in the plot to match the font in the text. Luckily, it is possible to use your LaTeX distribution to compile the labels of the figures. In this case, my examination office required me to use Times New Roman which is available for matplotlib but not for pdflatex. pdflatex has only Times Roman in the package newtxtext, but who will recognize the difference :).\n# These parameters can also be put into the style or matplotlibrc. # This is the dynamic approach of changing parameters. nice_fonts = { \u0026#34;text.usetex\u0026#34;: True, \u0026#34;font.family\u0026#34;: \u0026#34;serif\u0026#34;, \u0026#34;font.serif\u0026#34; : \u0026#34;Times New Roman\u0026#34;, } matplotlib.rcParams.update(nice_fonts) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) plt.show() plt.close()    The plot size The plot size is extremely critical because it also sets the frame for all other options. E.g. if we want to embed an image within our thesis and the fontsize of the labels should match the thesis text, we cannot scale the figure with [width=\\textwidth].\nFurthermore, we want the plot size to be visually appealing where the golden ratio is a good rule of thumb for the ratio between width and height. The golden ratio is approximately 1.618.\nThe following function is from Jack Walton‚Äôs blog and returns the correct figure height and width in inches (used in matplotlib) for a given figure width in points (LaTeX‚Äôs measurement).\ndef set_size(width, fraction=1): \u0026#34;\u0026#34;\u0026#34; Set aesthetic figure dimensions to avoid scaling in latex. Parameters ---------- width: float Width in pts fraction: float Fraction of the width which you wish the figure to occupy Returns ------- fig_dim: tuple Dimensions of figure in inches \u0026#34;\u0026#34;\u0026#34; # Width of figure fig_width_pt = width * fraction # Convert from pt to inches inches_per_pt = 1 / 72.27 # Golden ratio to set aesthetic figure height golden_ratio = (5 ** 0.5 - 1) / 2 # Figure width in inches fig_width_in = fig_width_pt * inches_per_pt # Figure height in inches fig_height_in = fig_width_in * golden_ratio return fig_width_in, fig_height_in At last, how do we know the \\textwidth in points inside our document? Just insert \\the\\textwidth at the position of the figure in your text and it will be shown in your PDF document after compilation. If you are on a page in landscape mode, use \\linewidth instead (thanks to John Kormylo for this).\nAssuming our thesis has text width of 400pt, the resulting figure looks like this.\nfig, ax = plt.subplots(figsize=set_size(400)) ax.plot(x, np.sin(x)) plt.show() plt.close()    Vector or raster graphics This is the last thing to know about graphics. matplotlib has many options to export graphics. You have probably used png, jpeg or pdf in the past. Just forget about the first two because they produce raster graphics instead of vector graphics. Vector graphics allow for almost infinite zooming whereas zooming into png and similar formats produces blurry and blocky pictures. pdf can be easily included with the general graphicx package of LaTeX.\nConclusion That‚Äôs it! This is everything you need for publication-ready plots with matplotlib :).\nReferences  https://jwalton.info/Embed-Publication-Matplotlib-Latex  ","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"d1bc0620ff5bd2a94fa59240a5058a8f","permalink":"https://tobiasraabe.github.io/post/matplotlib-for-publications/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/post/matplotlib-for-publications/","section":"post","summary":"An explanation on how to create publication-ready figures with matplotlib.","tags":["publication","python","matplotlib"],"title":"Matplotlib for Publications","type":"post"},{"authors":["Tobias Raabe"],"categories":["python"],"content":"Updated 16.08.2019. Better syntax, less errors and an example to show inlining of functions.\nWhy should I use them Both decorators are extremely powerful as Numba compiles the instructions of the decorated function to machine code. That makes them significantly faster than the same operation written with Numpy. Of course, this is also true for functions decorated with @jit, but the other two decorators enable other features like reduction, accumulation and broadcasting (explanation) which make operations on bigger matrices even faster.\nWhen should I use them ‚Äú[P]remature optimization is the root of all evil‚Äù (Donald Knuth in Computer Programming as an Art, p. 671). Before you start to optimize your implementation use tools like line_profiler or snakeviz to profile your code and find the real bottlenecks, not the ones you think about. Often it is sufficient to find better implementations from the standard library or rewrite parts such that they use Numpy or similar optimized frameworks. If you use Numpy, do not use loops but array operations. Use higher-dimensional arrays to gain performance by broadcasting. Avoid copies. Also, refactoring and rewriting parts of the code may not give direct speed improvements, but your code becomes more compact and you can single out expensive operations.\nImagine you have done this, but still there are some functions which take up the majority of runtime. Next, decide whether you need a function that operates on single elements of an array or on arrays.\nUsing the @vectorize decorator The @vectorize is for writing efficient functions which work on every element of an array. One useful application for me was to compute the probability of x under a normal distribution with mean mu and standard deviation sigma. There already exists an implemenation in scipy, but it is incredibly slow. Let us start to write our own implementation.\nimport numpy as np def get_prob_norm_dist(x, mu=0, sigma=1): return 1 / (np.sqrt(2 * np.pi) * sigma) * np.exp(-(x - mu) ** 2 / (2 * sigma ** 2)) We can compare our implementation with the scipy version to make sure it works as expected. Use the testing utilities from numpy.testing as the precision of floating point numbers will always differ to some extent. (Tip: Use doctest to document and test your function at the same time. Here is an example.)\nfrom scipy.stats import norm np.random.seed(42) a = np.random.randn(10) np.testing.assert_array_almost_equal(norm.pdf(a), get_prob_norm_dist(a), decimal=15) Now, create test data and measure the performance of our two methods.\na = np.random.randn(10000) %timeit norm.pdf(a) 647 ¬µs ¬± 47.4 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)  %timeit get_prob_norm_dist(a) 78.4 ¬µs ¬± 1.68 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)  Surprisingly, we are already faster by a factor of 8. At this point you should ask yourself again if further optimization is really necessary and whether your time is better spent anywhere else - remember Donald Knuth! In this example we will come down the rabbit hole a little bit more. The next step involves rewriting the function with the @vectorize decorator.\nThe major new thing is that the decorator requires a signature or even multiple signatures which define the input and output types of the function. The output type wraps the input types with round brackets. There are two ways to define the signature: declare the types of the arguments with types from Numba or declare the types with a string. I prefer the latter as it keeps your import statements short. With nopython=True, Numba does not use Python as a fallback if compilation fails, so we get notified. Also, the new function does not like default arguments.\nfrom numba import vectorize @vectorize([\u0026#34;float64(float64, float64, float64)\u0026#34;], nopython=True) def get_prob_norm_dist_fast(x, mu, sigma): return 1 / (np.sqrt(2 * np.pi) * sigma) * np.exp(-(x - mu) ** 2 / (2 * sigma ** 2)) %timeit get_prob_norm_dist_fast(a, 0, 1) 248 ¬µs ¬± 6.23 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)  The new version actually requires three times the runtime of the Numpy version. But, while using Numba we need to code a little bit different. As appealing the one-liner may look, Numba works better if we store intermediate results in other variables and combine them later.\n@vectorize([\u0026#34;float64(float64, float64, float64)\u0026#34;], nopython=True, target=\u0026#34;cpu\u0026#34;) def get_prob_norm_dist_fast(x, mu, sigma): y = x - mu a = np.sqrt(2 * np.pi) * sigma b = np.exp(-y ** 2 / (2 * sigma ** 2)) probability = 1 / a * b return probability %timeit get_prob_norm_dist_fast(a, 0, 1) 248 ¬µs ¬± 3.92 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)  We do not gain any performance, but this is just a showcase. If the operation does not involve many calculations and the input is not big, you cannot expect more performance than plain Numpy. If we increase the number of calculations and set the target keyword from cpu to parallel, we get similar ‚Ä¶","date":1555200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565913600,"objectID":"fc2e32c427085593b0813aacd57b70b7","permalink":"https://tobiasraabe.github.io/post/numba-guvectorize/","publishdate":"2019-04-14T00:00:00Z","relpermalink":"/post/numba-guvectorize/","section":"post","summary":"An explanation for the @vectorize and @guvectorize decorators from Numba.","tags":["numba","python"],"title":"Numba - @vectorize and @guvectorize","type":"post"},{"authors":["Tobias Raabe"],"categories":["python"],"content":"Introduction In one of my recent projects, I needed to accelerate a discrete choice dynamic programming model. After I changed a part of the implementation, the program was indeed faster. But, the most expensive operation according to profiling with snakeviz was now ~:0(\u0026lt;method \u0026#39;copy\u0026#39; of \u0026#39;numpy.ndarray\u0026#39; objects\u0026gt;). I was puzzled. I was sure that there was no use of np.copy() at all. After reading some StackOverflow posts and blog entries, it became clear that some operations and more importantly indexing methods return copies instead of views. The difference between the two is that views refer to the same underlying data in memory whereas a copy creates a new object. The disadvantages of a copy are:\n takes more time takes more memory  But, what operations return copies?\nHow to identify views and copies? To notice whether two objects do not refer to the same data buffer in the memory, we use the following function.\nimport numpy as np def aid(x): \u0026#34;\u0026#34;\u0026#34;This function returns the memory block address of an object.\u0026#34;\u0026#34;\u0026#34; return x.__array_interface__[\u0026#34;data\u0026#34;][0] Let us start simple. We construct an array and take a look at the memory block address of the array and the same array starting at the first position.\nx = np.array([1, 2, 3]) aid(x), aid(x[1:]) (2065100659856, 2065100659860)  Indeed, they have very similar addresses but the slice has an offset of 4. Every offset represents a byte so that the first number blocks 32bit. Furthermore, we know that the array contains integers. Thus, the dtype must be int32.\nx.dtype dtype(\u0026#39;int32\u0026#39;)  Addresses are only identical, if they share the same first element.\naid(x), aid(x[:2]) (2065100659856, 2065100659856)  As we are more interested in whether two objects come from the same memory block address instead whether they start at the same offset, we define two other functions.\ndef get_data_base(x): base = x while isinstance(base.base, np.ndarray): base = base.base return base def arrays_share_data(x, y): return get_data_base(x) is get_data_base(y) Just a quick test.\narrays_share_data(x, x.copy()) False  arrays_share_data(x, x[1:]) True  View vs. Copy Let us start examining when a copy or a view is returned.\nIn-place operations x = np.arange(16).reshape(4, 4) x_base = get_data_base(x) x *= 2 get_data_base(x) is x_base True  a = x * 2 arrays_share_data(x, a) False  Matrix multiplication x = np.arange(16).reshape(4, 4) x_base = get_data_base(x) x = x.dot(np.eye(4)) get_data_base(x) is x_base False  Indexing arrays_share_data(x, x[0]) True  arrays_share_data(x, x[0, 0]) False  arrays_share_data(x, x[:1, :1]) True  arrays_share_data(x, x[0][0]) False  This is a little bit mind-boggling, right? There is no problem with the first case as you probably expected that the two array share the same base. The other three cases all index the same element of the matrix, but in two of the cases a copy is returned. Why is that? The reason is that there are two kinds of indexing. The first group of indexing comprises simple indices, x[0], slices, x[:2] and boolean masks, x[x \u0026gt; 0]. These methods all return a view and not a copy. The second group is called fancy indexing and basically means that we use arrays of indices to access multiple values at once. The simplest way of fancy indexing is using lists of indices.\nIn the following example, a has just a different ordering of rows than x.\nx = np.arange(16).reshape(4, 4) x_base = get_data_base(x) a = x[[0, 2, 1, 3]] a array([[ 0, 1, 2, 3], [ 8, 9, 10, 11], [ 4, 5, 6, 7], [12, 13, 14, 15]])  arrays_share_data(x, a) False  We can also combine fancy indexing with other indexing schemes, but the return value is always a copy.\na = x[1, [1, 2]] a array([5, 6])  arrays_share_data(x, a) False  So every form of fancy indexing returns a copy. What about the following case?\na = x[(1,)] What do you expect?\narrays_share_data(x, a) True  And, this one?\nx = np.arange(16).reshape(4, 4, -1) a = x[(1, 2)] arrays_share_data(x, a) True  I was a little bit puzzled by this one at first as I thought it is the same as the following.\na array([6])  b = x[[1, 2]] b array([[[ 4], [ 5], [ 6], [ 7]], [[ 8], [ 9], [10], [11]]])  arrays_share_data(x, b) False  The reason is that ellipses are simply omitted and if the resulting index is not fancy the return value is a view.\nReferences  https://ipython-books.github.io/45-understanding-the-internals-of-numpy-to-avoid-unnecessary-array-copying/ http://www.jessicayung.com/numpy-views-vs-copies-avoiding-costly-mistakes/ https://jakevdp.github.io/PythonDataScienceHandbook/02.07-fancy-indexing.html  ","date":1553472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553472000,"objectID":"c4dac3a52982a46f9185940f0d2bcecc","permalink":"https://tobiasraabe.github.io/post/numpy-views-vs-copies/","publishdate":"2019-03-25T00:00:00Z","relpermalink":"/post/numpy-views-vs-copies/","section":"post","summary":"This article explains the difference between views and copies of a Numpy array.","tags":["numpy","python"],"title":"Numpy - Views vs. Copies","type":"post"},{"authors":["Tobias Raabe"],"categories":["python","economics"],"content":"The Roy model Roy (1951) provides the modern framework for modeling occupational choices as an earnings maximization problem.\nAssume that there are multiple sectors $j$ in the economy with $j=1, 2$. Individual skills, $S_j$, are drawn from a multivariate normal distribution. Each skill is associated with a price $\\pi_j$ per skill unit which leads to the following wage formula:\n$$\\begin{align*} \\log W_j \u0026amp;= \\log \\pi_j + S_j\\ \u0026amp;= \\log \\pi_j + \\mu_j + u_j \\end{align*}$$\nThe basic model does not include costs for working in one of the sectors. Considering an example where sectors represent different countries, costs stand for emigrating into the other country.\nimport matplotlib.pyplot as plt import numpy as np from scipy.stats import gaussian_kde number_of_agents = 1000 œÄ_1 = 1.0 œÄ_2 = 1.0 error_distribution_mean = np.zeros(2) error_distribution_corr = 0.25 error_distribution_covariance = np.array([ [1, error_distribution_corr], [error_distribution_corr, 1] ]) u_1, u_2 = np.random.multivariate_normal( error_distribution_mean, error_distribution_covariance, number_of_agents ).T Œº_1 = 5 Œº_2 = 5 S_1 = Œº_1 + u_1 S_2 = Œº_2 + u_2 wage_1_log = np.log(œÄ_1) + S_1 wage_2_log = np.log(œÄ_2) + S_2 is_working_in_sector_1 = np.where(wage_2_log \u0026lt; wage_1_log, True, False) plt.scatter( wage_1_log[is_working_in_sector_1], wage_2_log[is_working_in_sector_1], label=\u0026#34;Sector 1\u0026#34; ) plt.scatter( wage_1_log[~is_working_in_sector_1], wage_2_log[~is_working_in_sector_1], label=\u0026#34;Sector 2\u0026#34; ) plt.xlabel(\u0026#34;Log Wage Sector 1\u0026#34;) plt.ylabel(\u0026#34;Log Wage Sector 2\u0026#34;) plt.legend() plt.show() plt.close()    The selection into sectors affects the distribution of wages which are observed by researchers. We get the density of observed and all wages using a gaussian kernel. The different distributions are shown below. Note that the distribution has a higher mean than the distribution of all wages because people chose sector one only if it is optimal for them.\nkernel_1_observed = gaussian_kde(wage_1_log[is_working_in_sector_1]) kernel_1 = gaussian_kde(wage_1_log) positions = np.linspace(0, 10, 1000) plt.plot( positions, kernel_1_observed(positions), label=\u0026#34;Observed Wages\u0026#34; ) plt.plot(positions, kernel_1(positions), label=\u0026#34;All Wages\u0026#34;) plt.xlabel(\u0026#34;Wages\u0026#34;) plt.ylabel(\u0026#34;Density of Wages\u0026#34;) plt.legend() plt.show() plt.close()    kernel_2_observed = gaussian_kde(wage_2_log[~is_working_in_sector_1]) kernel_2 = gaussian_kde(wage_2_log) positions = np.linspace(0, 10, 1000) plt.plot( positions, kernel_2_observed(positions), label=\u0026#34;Observed Wages\u0026#34; ) plt.plot(positions, kernel_2(positions), label=\u0026#34;All Wages\u0026#34;) plt.xlabel(\u0026#34;Wages\u0026#34;) plt.ylabel(\u0026#34;Density of Wages\u0026#34;) plt.legend() plt.show() plt.close()    Further references:\n https://hceconomics.uchicago.edu/sites/default/files/5_HO_RoyModel_Econ350_SLIDES_2016-01-03a_jbb.pdf  ","date":1547856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547856000,"objectID":"b375cdc09ebf18a953346de3387221d0","permalink":"https://tobiasraabe.github.io/post/roy-model/","publishdate":"2019-01-19T00:00:00Z","relpermalink":"/post/roy-model/","section":"post","summary":"This article presents the Roy model, a framework for modelling occupational choices.","tags":["economics","python"],"title":"The Roy Model","type":"post"},{"authors":["Tobias Raabe"],"categories":["python","economics"],"content":"Solow-Swan model This is the basic model with constant labor inputs and discrete time.\nimport numpy as np import matplotlib.pyplot as plt s = 0.3 Œ¥ = 0.1 k_0 = 4 k_max = 10 def production_function(k): return k ** 0.5 def capital_accumulation_equation(s, Œ¥, k): return s * production_function(k) + (1 - Œ¥) * k k_star = (s / Œ¥) ** 2 k_t = np.arange(10) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4)) ax1.plot((0, k_max), (0, k_max), label=\u0026#34;45¬∞\u0026#34;, c=\u0026#34;black\u0026#34;) ax1.plot(k_t, capital_accumulation_equation(s, Œ¥, k_t), label=\u0026#34;Motion of capital\u0026#34;) ax1.plot((k_star,) * 2, (0, k_star), label=\u0026#34;$k^*$\u0026#34;, c=\u0026#34;grey\u0026#34;, ls=\u0026#34;--\u0026#34;) ax1.set_ylim(0) ax1.set_xlabel(\u0026#34;$k_t$\u0026#34;) ax1.set_ylabel(\u0026#34;$k_{t+1}$\u0026#34;) ax1.legend() ax2.plot(k_t, capital_accumulation_equation(s, Œ¥, k_t) - k_t, label=\u0026#34;Change in capital\u0026#34;) ax2.set_xlabel(\u0026#34;$k_t$\u0026#34;) ax2.set_ylabel(\u0026#34;$\\Delta k$\u0026#34;) ax2.set_ylim(0) ax2.legend() plt.tight_layout() plt.show() plt.close()    fig, ax = plt.subplots() ax.plot(k_t, production_function(k_t), label=\u0026#34;Production function\u0026#34;) ax.plot(k_t, s * production_function(k_t), label=\u0026#34;Investment\u0026#34;) ax.plot(k_t, Œ¥ * k_t, label=\u0026#34;Depreciated capital\u0026#34;) ax.plot((k_star,) * 2, (0, production_function(k_star)), label=\u0026#34;$k^*$\u0026#34;) ax.set_xlim(0, 10.5) ax.set_ylim(0, 3.5) ax.set_xlabel(\u0026#34;$k_t$\u0026#34;) ax.set_ylabel(\u0026#34;$f(k_t)$\u0026#34;) ax.legend() plt.show() plt.close()    Extension 1: continuous time We extend the model by implementing continuous time. We define\n$$ \\dot{x}_t = \\frac{d x_t}{dt} $$\nwhich is equivalent to the normal model when time is in units of one. The capital accumulation function changes to\n$$ \\dot{K} = s F[K_t, L_t] - \\delta K_t $$\nand in per capita form\n$$\\begin{align} \\dot{k} \u0026amp;= \\frac{\\dot{K}}{L_t}\\ \u0026amp;= sf[k_t] - \\delta k_t \\end{align}$$\nThe steady state requires $\\dot K = \\dot k = 0$ which leads to\n$$ \\frac{f(k^)}{k^{}} = \\frac{\\delta}{s} $$\nExtension 2: population growth Up to this point, we have assumed that the number of people working in the economy stays the same. Now, the population is growing with\n$$ L_t = L_0 e^{nt} $$\nso that the growth rate is $\\frac{\\dot{L}_t}{L} = n$. Thus, we have a new capital accumulation function per capita.\n$$\\begin{align} \\dot{k}_t \u0026amp;= \\frac{d\\frac{K_t}{L_t}}{d t}\\ \u0026amp;= \\frac{\\dot{K_t} L_t - K_t \\dot{L_t}}{L_t^2}\\ \u0026amp;= \\frac{\\dot K_t}{L_t} - \\frac{K_t}{L_t}\\frac{\\dot{L_t}}{L_t}\\ \u0026amp;= \\frac{s F[K_t, L_t] - \\delta K_t}{L_t} - n k_t\\ \u0026amp;= s f(k_t) - (\\delta + n) k_t \\end{align}$$\nExtension 3: technological growth Now, we add technological growth which enters the production function by improving the effectiveness of labor. This is also called Harrod-neutral progress or labor-augment technology.\n$$ Y_t = F(K_t, A_t L_t) $$\nTechnological growth suffice $A_t = A_0 e^{\\theta t}$ so that the growth rate is $\\frac{\\dot{A}_t}{A_t} = \\theta$.\nTo simplify the model, all terms will be expressed in per capita effective labor terms. The capital accumulation function changes to\n$$\\begin{align} \\dot{k}_t \u0026amp;= \\frac{d\\frac{K_t}{L_t A_t}}{d t}\\ \u0026amp;= \\frac{\\dot{K_t} L_t A_t - K_t (\\dot{L_t} A_t + L_t \\dot{A}_t)}{L_t^2 A_t^2}\\ \u0026amp;= \\frac{\\dot K_t}{L_t A_t} - \\frac{K_t}{L_t A_t}\\frac{\\dot{L_t}}{L_t} - \\frac{K_t}{L_t A_t} \\frac{\\dot{A}_t}{A_t}\\ \u0026amp;= \\frac{s F[K_t, L_t A_t] - \\delta K_t}{L_t A_t} - n k_t - \\theta k_t\\ \u0026amp;= s f(k_t) - (\\delta + n + \\theta) k_t \\end{align}$$\n","date":1547856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547856000,"objectID":"ba5a707758a1c391fd5992e11da6629e","permalink":"https://tobiasraabe.github.io/post/solow/","publishdate":"2019-01-19T00:00:00Z","relpermalink":"/post/solow/","section":"post","summary":"This is a basic implementation of the Solow model.","tags":["economics","python"],"title":"The Solow model","type":"post"},{"authors":["Tobias Raabe"],"categories":["python","economics"],"content":" If you are here for some examples of time series analysis in Julia, you are right where you belong. If you are looking for opinions on Julia and how it fits into the scientific computational landscape, you should read these posts:\n Why Numba and Cython are not substitutes for Julia by Christopher Rackauckas JAX vs Julia (vs PyTorch) by Patrick Kidger    As I had not used Julia before and only heard about how fast it is, that it is statically typed, and so on, I was very interested in the beginning, but that changed quickly.\nThe main cause of frustration was that the Julia developers released three versions during the time of the course. Version 0.6.4 was released on 9 July 2018, version 0.7.0 and 1.0.0 followed on 8 and 9 August respectively. All versions changed the main parts of the API and probably the last change made most standard modules importable. As much as I appreciate the effort of making a language more modular and optimizing the structure, the changes probably broke all code examples on the web which totally clashes with my way of learning a new language by copying and pasting parts together. At least, one can assume that the changes will be minor in the future after the release of 1.0.0.\nAnother disadvantage is that Julia cannot be shipped with conda on Windows which is still my platform of choice, but maybe I am the one to blame here üòÑ.\nApart from that, I do not know much about Julia and my current critique is more about convenience which comes with a more mature language. Comparing a 27-year-old to a 6-year-old is unfair.\nThe main selling points of Julia are speed and its mixture of dynamic and static typing. Regarding the first item, Python can be brought to comparable speed by using CPython or Numba, but all the benchmarks always depend on whether you hold the implementation over all languages constant or whether you optimized the problem for each language and whether there is a huge difference between a simple program and large implementation. So, I am not very sure what the final answer is here. Furthermore, all projects are heavily under development and benchmarks are quickly outdated. Regarding static typing, I think it would make some data analyses more consistent and currently, there is no real match in Python.\nMaybe I will use Julia in the future, but currently, I am too comfortable with Python and do not have a real use case where I am forced to switch to something else. Path dependency is a bitch.\n","date":1539734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539734400,"objectID":"ad580a4d642dca229d92251fd348cb10","permalink":"https://tobiasraabe.github.io/post/time-series/","publishdate":"2018-10-17T00:00:00Z","relpermalink":"/post/time-series/","section":"post","summary":"A time series course with Julia.","tags":["Research","time series","economics"],"title":"A time series course with Julia","type":"post"},{"authors":["Tobias Raabe"],"categories":["research"],"content":"In 2015, I wrote my Bachelor‚Äôs thesis on identifying software patents. This is useful and necessary in two ways. First, there is no official system to sort patents this way. The main system used by the USPTO focuses on the technological and functional form. A subclass dealing with dispensing solids contains manure spreaders and toothpaste tubes. In contrast, researchers are more interested in topics like automation or software. Second, I learned Python and made my first steps into the world of machine learning.\nYou can find the whole project on Github as well as the paper. There is also a script to download different kinds of data sets. The raw data uses approximately 90GB of disk space whereas the data for replicating the previous results based on a simple algorithm is currently less than 1GB.\nNow, let us see what has been done so far.\nThe idea of the project is based on an algorithmic approach of Bessen \u0026amp; Hunt (2007). The authors wanted to estimate the number of software patents and find out where software patents are used and what economic indicators are correlated with having more software patents.\nTo classify patents into categories of software and non-software, the authors developed a simple algorithm based on the evaluation of a random sample of patents. The algorithm is as follows:\n((\u0026#34;software\u0026#34; in specification) OR (\u0026#34;computer\u0026#34; AND \u0026#34;program\u0026#34; in specification)) AND (utility patent excluding reissues) ANDNOT (\u0026#34;chip\u0026#34; OR \u0026#34;semiconductor\u0026#34; OR \u0026#34;bus\u0026#34; OR \u0026#34;circuit\u0026#34; OR \u0026#34;circuitry\u0026#34; in title) ANDNOT (\u0026#34;antigen\u0026#34; OR \u0026#34;antigenic\u0026#34; OR \u0026#34;chromatography\u0026#34; in specification)  According to Bessen \u0026amp; Hunt (2007), the specification is the abstract and the description of the patent. Note that, PatentsView separates description into two parts, summary and description.\nTo replicate the algorithm, the project relies on two strategies. The first data source is Google Patents where the texts can be crawled. As this procedure is not feasible for the whole corpus of patents, the second data source is PatentsView which provides large data files for all patents from 1976 on.\nThe replication of the original algorithm succeeds in 398 of 400 cases as one patent was retracted and in one case an indicator was overlooked which lead to an error in the classification.\nCompared to the manual classification of the authors, the algorithm performed in the following way:\n    Relevant Not Relevant     Retrieved 42 8   Not Retrieved 12 337    Relevant refers to software patents according to the manual classification whereas retrieved indicates software patents detected by the algorithm. The upper left corner can also be called true-positives whereas the lower right corner shows the number of true-negatives.\nApplying the algorithm to the whole patent corpus, we get the following distributions of patents and software versus non-software patents.\n Absolute Number of Utility Patents   Absolute Number of Software vs. Non-Software Patents   Relative Number of Software vs. Non-Software Patents  ","date":1534809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534809600,"objectID":"bc4aa0257a54e24a161e94fe6107d1b4","permalink":"https://tobiasraabe.github.io/post/software-patents/","publishdate":"2018-08-21T00:00:00Z","relpermalink":"/post/software-patents/","section":"post","summary":"A research project for identifying software patents.","tags":["Research","Software","Patents"],"title":"Identifying Software Patents","type":"post"},{"authors":["Tobias Raabe"],"categories":["python"],"content":" A lot of time has passed since this post. Meanwhile I wrote a tool called pytask ‚Äì a replacement for Waf ‚Äì which allows you to execute the workflows in research project. It also comes with two recommended templates.\n cookiecutter-pytask-project is a lean template. econ-project-templates is richer and specifically designed for economists.    In one of the university courses I was introduced to a Waf framework for reproducible research by Hans-Martin von Gaudecker which is amazingly useful to manage your research project.\nThe basic idea is that a project is structured as a DAG, a directed-acyclic graph. A DAG is a graph with a finite amount of node and edges where the edges have a specific direction leading from input to output files. Furthermore, starting at node $\\nu$ and following the directed edges, it is not possible to find a way back to $\\nu$.\nTake a look at the picture at the top. get_simulation_draws.py is the starting point and the source of initial_locations.csv which is the input of ‚Ä¶ You get it.\nYou get the sample project by installing cookiecutter first with\n$ pip install -U cookiecutter Then, go to the directory which should contain the folder with the project and type\n$ cookiecutter https://github.com/tobiasraabe/cookiecutter-research-template.git Answer the following prompts so the project will be customized to your needs.\nIn the end, go into the project folder and set up the conda environment.\n$ conda env create -n \u0026lt;project-name\u0026gt; -f environment.yml At last, run the following command to make sure that the sample project works.\n$ python waf.py distclean configure build For more information on Waf read Gaudecker‚Äôs documentation and my Waf Tips \u0026amp; Tricks.\nTo use all features of the template check out the documentation.\n","date":1531785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531785600,"objectID":"fd50c02504c1f29589b2efe367c0f663","permalink":"https://tobiasraabe.github.io/post/cookiecutter-research-template/","publishdate":"2018-07-17T00:00:00Z","relpermalink":"/post/cookiecutter-research-template/","section":"post","summary":"A research template for reproducible research.","tags":["Research","Reproducibility"],"title":"Facilitate reproducible research with cookiecutter-research-template","type":"post"},{"authors":["Tobias Raabe"],"categories":["python"],"content":"In one of my recent projects on natural language processing and patents, I tried to program a little script which downloads files needed for the analysis.\nThe program should be able to do the following things:\n Download a given file Resume the download if the file is incomplete Validate the file at the end of the download using hashes Wrap everything with beautiful progress bars  Note that, to run the script you have to have at least a python 3.6 environment as I am using f-strings.\nDownloading files At first, we need to know how to download a file. We will use requests to handle connections to web content.\nimport requests In the next step, we will download a file and store it on the disk. The file size is 19kb, so you are safe to download it. If you have doubts about the file, insert any other link which can be a picture or something else.\nurl = \u0026#39;http://www.patentsview.org/data/20171226/cpc_group.tsv.zip\u0026#39; r = requests.get(url) with open(\u0026#39;cpc_group.tsv.zip\u0026#39;, \u0026#39;wb\u0026#39;) as f: f.write(r.content) Note that requests.get will download the file immediately and store it in memory. For larger files, this does not work as they might not fit in memory. Therefore, we use the stream keyword to receive the file in chunks and store them on disk. The next file is about 3MB big. Use a different URL if you feel more comfortable with that.\nurl = \u0026#39;http://www.patentsview.org/data/20171226/government_interest.tsv.zip\u0026#39; r = requests.get(url, stream=True) with open(\u0026#39;government_interest.tsv.zip\u0026#39;, \u0026#39;wb\u0026#39;) as f: for chunk in r.iter_content(32 * 1024): f.write(chunk) Resuming downloads If you download large files, chances are that your download is interrupted. For that, we need a way to resume the download at the last byte position. We can do that by sending a Range-request to the server and specifying the range of bytes, we want to receive.\nresume_header = ({\u0026#39;Range\u0026#39;: f\u0026#39;bytes=0-2000000\u0026#39;}) r = requests.get(url, stream=True, headers=resume_header) with open(\u0026#39;government_interest_part.tsv.zip\u0026#39;, \u0026#39;wb\u0026#39;) as f: for chunk in r.iter_content(32 * 1024): f.write(chunk) We look at the file size whether we have been able to download the file partially.\nfrom pathlib import Path path = Path(\u0026#39;government_interest_part.tsv.zip\u0026#39;) print(f\u0026#39;Size of the file: {path.stat().st_size}Bytes\u0026#39;) Size of file: 2000001 Bytes  Now, we resume the download at the position of the last byte. We also have to change the mode from open() to \u0026#39;ab\u0026#39; as we are appending new content and do not want to overwrite existing content.\nresume_header = ({\u0026#39;Range\u0026#39;: f\u0026#39;bytes={path.stat().st_size}-\u0026#39;}) r = requests.get(url, stream=True, headers=resume_header) with open(\u0026#39;government_interest_part.tsv.zip\u0026#39;, \u0026#39;ab\u0026#39;) as f: for chunk in r.iter_content(32 * 1024): f.write(chunk) The crucial aspect of this code is that we correctly define the following range of bytes for the following downloads. It is important to know that the boundaries of the Range header are inclusive so that bytes in positions 0 and 2000000 are downloaded. Therefore, the file size is 2000001 and we can plug in the value to correctly resume the download.\nValidating downloads At last, we want to make sure that the download of the file worked as expected. To assert whether two files are identical, one of the easiest solutions is to compare hashes. A hash function projects any kind of data onto data with fixed-length. The resulting object is called hash and should be different for two different objects but identical for the same. In this example, we use SHA256 to validate the two files.\nimport hashlib with open(\u0026#39;government_interest.tsv.zip\u0026#39;, \u0026#39;rb\u0026#39;) as f: content = f.read() sha = hashlib.sha256() sha.update(content) print(sha.hexdigest()) 42e53da0f2adc03e035eb2f967998da5cb8e2b1235cd2630efc59e31df866372  with open(\u0026#39;government_interest_part.tsv.zip\u0026#39;, \u0026#39;rb\u0026#39;) as f: content = f.read() sha = hashlib.sha256() sha.update(content) print(sha.hexdigest()) 42e53da0f2adc03e035eb2f967998da5cb8e2b1235cd2630efc59e31df866372  Both hashes match and we have correctly resumed the download :).\nVisualizing download progress There are multiple options to print pretty progress bars in python to the command line interface (progressbar2 e.g.), but I used tqdm for unknown reasons.\nTo display the progress of the download, we have to know the total file size which we can easily request without downloading the whole file. Use requests.head() instead of requests.get().\nurl = \u0026#39;http://www.patentsview.org/data/20171226/government_interest.tsv.zip\u0026#39; r = requests.head(url) file_size = int(r.headers.get(\u0026#39;content-length\u0026#39;, 0)) print(f\u0026#39;Size of file: {file_size}\u0026#39;) 3895459  Note that content length returns a string and not a number.\nfrom tqdm import tqdm r = requests.get(url, stream=True) initial_pos = 0 with open(\u0026#39;government_interest.tsv.zip\u0026#39;, \u0026#39;wb\u0026#39;) as f: with tqdm(total=file_size, unit=\u0026#39;B\u0026#39;, unit_scale=True, unit_divisor=1024, desc=\u0026#39;government_interest.tsv.zip\u0026#39;, initial=initial_pos, ascii=True, miniters=1) as pbar: for chunk in r.iter_content(32 * 1024): f.write(chunk) ‚Ä¶","date":1528675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528675200,"objectID":"9d35d5aac789d6e224eb42c1c343b491","permalink":"https://tobiasraabe.github.io/post/how-to-download-files-with-python/","publishdate":"2018-06-11T00:00:00Z","relpermalink":"/post/how-to-download-files-with-python/","section":"post","summary":"A short script in python to download files, resume and to validate downloads with hash values.","tags":["python","Download"],"title":"How to download files with Python","type":"post"},{"authors":["Tobias Raabe"],"categories":["packaging"],"content":" This guide is outdated. Today, I would recommend to publish a package on conda-forge so that it will be available for the whole community. Here is the guide to do that: https://conda-forge.org/docs/maintainer/adding_pkgs.html.   This article shows how to compile and distribute R packages on anaconda.org to be used in your data science projects. This is useful as R has not really a neat dependency pinning tool like Python with requirements.txt or environment.yml with conda and R is shipped with conda anyway. But, if you want to use the MKL accelerated Microsoft R Open instead of plain R, there are some packages which are currently not provided in conda‚Äôs default channels or conda-forge. Here is how to lift this obstacle.\nIntroduction to Anaconda and R I like to manage my research projects with conda which is the package manager for Anaconda, a popular Python distribution for data science. For one of my recent projects, I also needed to install R and I was lucky to find out that R is also available with conda.\nFirst, you create your normal Python environment for your new project\n$ conda create --name project python=3.6 anaconda This installs a complete Anaconda distribution with Python 3.6 under the name project. If you only want the bare Python interpreter, call\n$ conda create -n project python=3.6 Activate the environment with\n$ activate project  Hint: Since my OS is Windows, I prefer to use Powershell. Unfortunately, activating and deactivating your environment usually fails. Try out pscondaenvs which installs corrected Powershell scripts.\n If we also need an R distribution for our project, we have to make the decision between to R from R-Project and MRO from Microsoft. The latter is the default with Anaconda, but the former is still provided if you are running a 32-bit operating system or an older macOS version. The advantage of MRO is that it is using the Intel Math Kernel Library (MKL) and enables multithreading by default (here are some benchmark reports and information on how to set the number of threads used).\nInstalling a basic R interpreter from MRO is as simple as typing\n$ conda install --channel r mro-base If you want to install R from R-Project, type conda install -c r r-base.\nIt is important to note that it is impossible to have both R interpreters in one environment and that packages for either one of them do not work with the other. You can find more information here.\nThere is also the option to install a whole R distribution which is called r-essentials. It bundles many useful packages along with IRkernel which enables you to use R in Jupyter notebooks. Again, there are two commands depending on which R interpreter you are using.\n$ conda install -c r r-essentials $ conda install -c r r-essentials r-base To update all of your R packages run\n$ conda update r-essentials If your desired package is not available in r-essentials, you can use the search on https://anaconda.org to find a channel which offers your package. But what if your package is not available?\nBuilding and Distributing an R package I had the same issue when I wanted to use mice which is a known framework for multiple imputations by chained equations.\nTo build the package for your conda distribution, invoke the following command\n$ conda skeleton cran r-mice This will create a folder called r-mice which contains three files, bld.bat, build.sh and meta.yml. meta.yml is the important file which controls the compilation. An example can be found here. Note the key called requirements. inside host and run the R interpreter is defined. By default, it is r-base. If you are using MRO, you have to change this to mro-base.\nrequirements:build:- {{compiler(\u0026#39;c\u0026#39;) }} # [not win]- {{compiler(\u0026#39;cxx\u0026#39;) }} # [not win]- {{native}}toolchain # [win]- {{posix}}filesystem # [win]- {{posix}}makehost:- r-base- r-mass- r-rcpp- r-lattice- r-nnet- r-rpart- r-survivalrun:- r-base- {{native}}gcc-libs # [win]- r-mass- r-rcpp- r-lattice- r-nnet- r-rpart- r-survivalIn the next step, we want to compile the package. If you are on Windows, make sure to install RTools in advance and add binaries to your system‚Äôs path during the installation. To compile the package, type\n$ conda build r-mice After the compilation ended successfully, you can install the package with\n$ conda install --use-local r-mice You can also upload the package to Anaconda.org to your private repository and make it accessible to all people.\nThat‚Äôs what I did. I have created a repository which builds the package for Linux and macOS with Travis-CI and Windows with Appveyor. The compiled packages are uploaded to my account and can be installed via\n$ conda install -c brimborium r-mice Compile your own package If you want to use my solution for yourself, fork my repository. Then, replace the recipe in conda-recipe with your recipe create with\n$ conda skeleton cran \u0026lt;package name\u0026gt; Push the repository to your Github account and create accounts on Travis- CI, Appveyor and Anaconda.org.\nNext, go to ‚Ä¶","date":1521590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521590400,"objectID":"d0ab2a9b35581b7875e64ae1e9766130","permalink":"https://tobiasraabe.github.io/post/distribute-r-conda/","publishdate":"2018-03-21T00:00:00Z","relpermalink":"/post/distribute-r-conda/","section":"post","summary":"This article shows how to compile and distribute R packages on anaconda.org.","tags":["conda","r","packaging"],"title":"How to compile and distribute R packages with conda","type":"post"},{"authors":["Tobias Raabe"],"categories":["python"],"content":"Introduction This Jupyter notebook uses the Titanic dataset from Kaggle for the prediction of survival rates among passengers on the Titanic.\n RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning of 15 April 1912, after colliding with an iceberg during her maiden voyage from Southampton to New York City. Of the 2,224 passengers and crew aboard, more than 1,500 died in the sinking, making it one of the deadliest commercial peacetime maritime disasters in modern history. The largest ship afloat at the time it entered service, the RMS Titanic was the second of three Olympic class ocean liners operated by the White Star Line, and was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster.\n The main goals of this analysis are a descriptive analysis of the data, a sophisticated feature engineering approach, and a comparison of established prediction models. In addition to that, the whole analysis should be presented in the best possible way, so that the notebook serves as a best practice example.\n%matplotlib inline import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;, category=FutureWarning) import matplotlib.patches as mpatches import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import seaborn as sns from time import time from pathlib import Path from scipy.stats import randint as sp_randint from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score from sklearn.model_selection import RandomizedSearchCV sns.set_style(\u0026#39;whitegrid\u0026#39;) np.random.seed(123) train = pd.read_csv(Path(\u0026#39;data\u0026#39;, \u0026#39;train.csv\u0026#39;)) test = pd.read_csv(Path(\u0026#39;data\u0026#39;, \u0026#39;test.csv\u0026#39;)) train.Embarked = train.Embarked.map({ \u0026#39;S\u0026#39;: \u0026#39;Southampton\u0026#39;, \u0026#39;C\u0026#39;: \u0026#39;Cherbourg\u0026#39;, \u0026#39;Q\u0026#39;: \u0026#39;Queenstown\u0026#39; }) train.Sex = train.Sex.map({\u0026#39;female\u0026#39;: \u0026#39;Female\u0026#39;, \u0026#39;male\u0026#39;: \u0026#39;Male\u0026#39;}) test.Embarked = test.Embarked.map({ \u0026#39;S\u0026#39;: \u0026#39;Southampton\u0026#39;, \u0026#39;C\u0026#39;: \u0026#39;Cherbourg\u0026#39;, \u0026#39;Q\u0026#39;: \u0026#39;Queenstown\u0026#39; }) test.Sex = test.Sex.map({\u0026#39;female\u0026#39;: \u0026#39;Female\u0026#39;, \u0026#39;male\u0026#39;: \u0026#39;Male\u0026#39;}) Descriptive Analysis For exploration the whole dataset is used. It consists of 1309 observations and 12 variables. The variables are:\n   Variable Description dtype non-missing train non-missing test     Age Age of passenger float64 714 332   Cabin Cabin of passenger object 204 91   Embarked Capital letter indicating the port of embarkation object 889 417   Fare Price of ticket float64 891 418   Name Name of passenger object 891 418   Parch Number of parents \u0026amp; children aboard int64 891 418   PassengerId ID of passenger int64 891 418   Pclass Class of passenger int64 891 418   Sex Sex of passenger object 891 418   SibSp Number of siblings \u0026amp; spouses aboard int64 891 418   Survived Survived (1) or deceased (0) int64 891 418   Ticket Ticket number object 891 418    PassengerId, Ticket, Cabin These variables are not important for the analysis.\nPassenger Class fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(18, 6)) sns.countplot(x=\u0026#39;Pclass\u0026#39;, data=train, ax=axis1) sns.barplot(x=\u0026#39;Pclass\u0026#39;, y=\u0026#39;Survived\u0026#39;, data=train, ax=axis2, ci=None) plt.show()    Sex fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(18, 6)) sns.countplot(x=\u0026#39;Sex\u0026#39;, data=train, ax=axis1) sns.barplot(x=\u0026#39;Sex\u0026#39;, y=\u0026#39;Survived\u0026#39;, data=train, ax=axis2, ci=None) plt.show()    Age # Create a dummy for missing age values for later computation train[\u0026#39;missing_age\u0026#39;] = train.Age.isnull() test[\u0026#39;missing_age\u0026#39;] = test.Age.isnull() # Missing computation (could be more sophisticated, since it is only # uniformly distributed) train_age_mean = train.Age.mean() train_age_std = train.Age.std() train_age_nan_count = train.Age.isnull().sum() test_age_mean = test.Age.mean() test_age_std = test.Age.std() test_age_nan_count = test.Age.isnull().sum() train_age_rand = np.random.randint( train_age_mean - train_age_std, train_age_mean + train_age_std, size=train_age_nan_count) test_age_rand = np.random.randint( test_age_mean - test_age_std, test_age_mean + test_age_std, size=test_age_nan_count) # Original distribution orig_train_age_int = train.Age.dropna().astype(int) orig_test_age_int = test.Age.dropna().astype(int) # Computed age distribution train.loc[train.Age.isnull(), \u0026#39;Age\u0026#39;] = train_age_rand comp_train_age_int = train.Age.astype(int) test.loc[test.Age.isnull(), \u0026#39;Age\u0026#39;] = test_age_rand comp_test_age_int = test.Age.astype(int) fig, (axis1, axis2) = plt.subplots(2, 1, figsize=(18, 8)) axis1.set_title(\u0026#39;Original Age Distribution\u0026#39;) sns.distplot( orig_train_age_int, ax=axis1, kde=False, bins=np.arange(0, 80.5, 1)) axis2.set_title(\u0026#39;Computed Age Distribution\u0026#39;) sns.distplot( comp_train_age_int, ax=axis2, kde=False, bins=np.arange(0, 80.5, 1)) plt.tight_layout() plt.show() # Convert Age from float to int, because of simplicity train.Age = train.Age.astype(int) test.Age = test.Age.astype(int)    fig, ax = plt.subplots(figsize=(18, 6)) sns.barplot(\u0026#39;Age\u0026#39;, \u0026#39;Survived\u0026#39;, data=train, ax=ax, ci=False) plt.show()    Fare # Could be better, ‚Ä¶","date":1508630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508630400,"objectID":"c23020df1424e7a6aea4738135c4de8e","permalink":"https://tobiasraabe.github.io/post/tragedy-of-titanic/","publishdate":"2017-10-22T00:00:00Z","relpermalink":"/post/tragedy-of-titanic/","section":"post","summary":"This is an analysis of survival rates on the Titanic with a placebo test for whether traveling in couples increased the likelihood of survival.","tags":["python","titanic"],"title":"The tragedy of Titanic","type":"post"}]